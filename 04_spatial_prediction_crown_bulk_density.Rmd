---
title: "Modeling Crown Bulk Density via Ridge Regression"
author: "Johannes Heisig"
output: github_document
---

This analysis predicts Crown Bulk Density, a canopy fuel variable relevant to crown fire spread. It is difficult to sample in the field and therefore estimated using measurements of other forest structure variables and allometric equations. Predictor data for this regression analysis comes from airborne LiDAR and Sentinel-1 & -2.

> NOTE:
> Binder is great for reproducing analysis in R. However, it has RAM restrictions, which can be a problem for large remote sensing data stacks. Below we provide switches for this program, which allow you to decide, whether you want to run all analysis steps or skip some of the computationally expensive ones (e.g. extracting raster values). Be prepared that Binder may crash when running complex tasks. If you run this script on Binder we suggest you stick with the switch settings below (skip.. = TRUE), which allows intermediate results to be loaded from existing files. Alternatively, you may choose to download this repository and run operations locally instead. In this case feel free to change switches below TRUE to FALSE.

Analysis switches:
```{r switches}
skip_training_data_extraction = TRUE
skip_model_prediction = TRUE
```

```{r}
suppressPackageStartupMessages({
library(glmnet)
library(caret)
library(CAST)
library(dplyr)
select = dplyr::select
library(tidyr)
library(ggplot2)
library(stars)
library(sf)
library(parallel)
library(doParallel)
library(patchwork)
})
knitr::opts_chunk$set(cache=T, warning = F, message = F)
```

## Prepare training and predictor data

### Read

```{r warning=FALSE}
if (skip_training_data_extraction){
  haard = readRDS(file.path(getwd(),"data",
                            "cbd_training_data.rds")) |> 
    st_sf() |> 
    select(-c(PlotID, FC_CBH, FC_SH, FC_CC))
} else {
  
  predictors_path = file.path(getwd(),"data","predictors_haard_10m.rds")
  if (! file.exists(predictors_path)){
    download.file("https://uni-muenster.sciebo.de/s/XPEk2uBClq2v3ob/download",
                  predictors_path)
  } else print("Predictor data already on disk.")
  
  p = readRDS(predictors_path) 
  dim(p)
  
  haard = st_read(file.path(getwd(),"data",
                            "haard_field_plot_locations.csv"), 
                  crs=4326, quiet=T,
                  options=c("X_POSSIBLE_NAMES=lon",
                            "Y_POSSIBLE_NAMES=lat")) |>
    select(plot_id, dom_spp)
  
  FC = read.csv(file.path(getwd(),"data","haard_cbd_fuelcalc.csv"))[-1,] |> 
    select(PlotID, preCBD, preCBH, preSH, preCC) |> 
    mutate(PlotID = sub(x=PlotID, "-Inventory", "") |> as.numeric(),
           across(2:5, as.numeric)) |> 
    rename_with(~sub("pre","FC_",.x)) |> 
    drop_na() |> 
    glimpse()
}

ft = read_stars(file.path(getwd(),"results","haard_surface_fuel_map.tif")) |> 
  setNames("FuelType")
```

### Extract

```{r warning=FALSE}
if (!skip_training_data_extraction){
  # Create buffer around visited field plots according to survey protocol
  haard = haard[haard$plot_id %in% FC$PlotID,] |> 
    st_transform(st_crs(p)) |> 
    st_buffer(10)
  
  #' Convert raster data to polygon geometries to enable
  #' intersection rather than pixel-based extraction.
  #' Extract area-weighted values from predictors at plot
  #' locations. First intersect buffers with predictors,
  #' then weight each fragment of a field plot by its 
  #' area in relation to plot size (pi*(10m)² = 314 m²)
  
  haard = st_intersection(st_as_sf(p), haard)
  haard = haard |> 
    mutate(across(-c(plot_id, dom_spp, geometry), 
                  ~as.numeric(.x * st_area(haard)/314))) |> 
    group_by(plot_id) |> 
    summarise(across(where(is.numeric), 
                     function(s) round(sum(s),2)), 
              dom_spp = unique(dom_spp))
  
  # join with target variable (CBD) based on field plot ID
  haard = FC |> 
    mutate(plot_id = as.character(PlotID)) |> 
    select(plot_id, FC_CBD) |> 
    inner_join(haard, by = "plot_id") |> 
    select(-plot_id, -dom_spp) |> 
    st_sf()
}

haard = st_drop_geometry(haard)

# CBD field samples
plot(1:nrow(haard), haard$FC_CBD, col="red")
```


### Fill NAs

```{r message=FALSE, warning=FALSE}
any(colSums(is.na(haard)) > 0) # no NAs in training point data

if (!skip_training_data_extraction){
  # how many NAs in which predictor?
  p.nas = colSums(is.na(as.data.frame(split(p))))
  p.nas[p.nas > 0] 
  
  # ==> NAs mainly in Z-related variables --> can be filled with zeros 
  
  # where in space are NAs?
  p.nas.st = st_apply(p, 1:2, function(x) sum(is.na(x)))
  plot(p.nas.st, breaks = "equal", 
       col = rev(terrain.colors(10)),
       main = "\nNumber of\nmissing predictor values per pixel")
  
  # ==> almost exclusively in non-forest areas --> less of a problem
  
  p = p %>% replace(is.na(.), 0)
}
```


### Split into training and test sets

```{r}
set.seed(111)
training = haard |> sample_frac(0.7)
testing = haard |> setdiff(training)

x_train = model.matrix(FC_CBD ~ ., training)[,-1]   # predictors
x_test = model.matrix(FC_CBD ~ ., testing)[,-1]

y_train = training$FC_CBD                           # target
y_test = testing$FC_CBD
```


### Remove zero variance predictors

```{r message=FALSE, warning=FALSE}
(zerovar = which(apply(scale(x_train), 2, FUN = function(x) {all(is.na(x))})))

x_train = x_train[,-zerovar]
x_test = x_test[,-zerovar]
if (!skip_training_data_extraction) p = p[,,,-zerovar] 
```

## Ridge regression with `caret` and `glmnet`

Prepare model training components:

-   A simple 5-fold random CV is used for assessment of model performance

-   Parameter `lambda` needs to be tuned through cross-validation. We use a tune grid with 100 possible values between 100 and 0.1.

-   Alpha (`a`) determines whether the model performs Ridge regression (`a = 0`), a Lasso regression (`a = 1`), or an Elasticnet regression (`0 < a > 1`).

-   Predictor variables (`p`) are translated from a `stars`-object to a matrix for model prediction.

```{r}
tc = trainControl("cv", 5)
grid = 10^seq(2, -1, length = 100)
a = 0
if (!skip_training_data_extraction){
  newx = p |> 
    split(3) |> 
    as.data.frame() |> 
    select(-x,-y) |> 
    as.matrix()
}
```

### Ridge regression log(CBD)

The `caret` modeling framework will be used as a wrapper for `glmnet`.

```{r}
set.seed(111)
glm_ridge = train(
  x_train,
  log(y_train),
  method = "glmnet",
  preProcess = c("center", "scale"),
  tuneGrid = data.frame(lambda=grid, alpha=0), 
  trControl = tc,
  importance = T)

#saveRDS(glm_ridge, "results/models/model_log_cbd_ridge.rds")
```

### Validation log(CBD)

Performance metrics
```{r}
best_lambda = glm_ridge$bestTune$lambda

modelpred = glm_ridge |> 
  predict(s = best_lambda) |> 
  exp()
testpred = glm_ridge |> 
  predict(s = best_lambda, newdata = x_test) |> 
  exp() 

print(paste("R2 (model prediction vs training data) =", R2(modelpred, y_train)))
print(paste("R2 (prediction vs validation data) =", R2(testpred, y_test)))

print(paste("RMSE (model prediction vs training data) =", RMSE(modelpred, y_train)))
print(paste("RMSE (prediction vs validation data) =", RMSE(testpred, y_test)))
print(paste("RMSE (intercept only model) =",RMSE(mean(y_train), y_test)))
```

Variable importance
```{r}
plot(varImp(glm_ridge), top=20) 
```

Residuals
```{r}
shapiro.test(residuals(glm_ridge))
hist(residuals(glm_ridge))
```

Scatter plot
```{r}
plot(log(modelpred), log(y_train), 
     xlim=c(-4,-1), ylim=c(-4,-1))
points(log(testpred),log(y_test), col="red")
abline(0,1)
```

Coefficients
```{r}
lambda_index = which(glm_ridge$results$lambda == best_lambda)
coef(glm_ridge$finalModel)[,lambda_index] |> 
  sort(decreasing = T)
```

## Prediction log(CBD)

-   Predict and back-transform to normal scale.

-   Set CBD of non-burnable areas to NA.

-   Calculate the Area of Applicability (AOA).

-   Finally, 117 values (0.013 %) ranging between 0.3 and 10^13 remain. They are considered to be unreasonable for CBD and are also set to NA.

```{r}
if(skip_model_prediction){
  (cbd_glm_st = read_stars(file.path(getwd(),"results","haard_CBD.tif")) |> 
     setNames("CBD"))
} else {
  cbd_glm = predict(glm_ridge, newdata = newx) |> 
    exp() |> 
    setNames("CBD")
  
  cbd_glm_st = ft |> 
    mutate(CBD = cbd_glm,   
           CBD = case_when(FuelType == 4 ~ NaN,   
                           CBD > 0.3 ~ NaN,
                           TRUE ~ CBD)) |> 
    select(CBD)
}

# Distribution of predicted CBD
hist(cbd_glm_st, breaks=100)
```

AOA
```{r}
if(skip_model_prediction){
  (AOA = read_stars(file.path(getwd(),"results","AOA_haard_CBD.tif")) |> 
     setNames("AOA"))
} else {
  cl <- makeCluster(6)
  registerDoParallel(cl)
  
  AOA = aoa(model=glm_ridge, newdata=p, cl = cl)
  
  stopCluster(cl)
}
```

Plot
```{r, cache=FALSE}
pcbd = ggplot() + 
  geom_stars(data = cbd_glm_st, downsample = 1) +
  coord_fixed() + 
  scale_x_discrete(expand = c(0, 0), name="") +
  scale_y_discrete(expand = c(0, 0), name="") +
  scale_fill_viridis_c(direction = -1) +
  theme(legend.position = "bottom")

AOA = c(AOA, cbd_glm_st) |> 
  mutate(AOA = ifelse(is.na(CBD), 99, AOA) |> 
           factor(levels = c(1,0,99),
                  labels = c("inside","outside","NB")))

paoa = ggplot() + 
  geom_stars(data = AOA["AOA"], downsample = 2) +
  coord_equal() + 
  scale_x_discrete(expand = c(0, 0), name="") +
  scale_y_discrete(expand = c(0, 0), name="") +
  scale_fill_manual(values=c("lightgreen","firebrick","grey80"), name = "AOA") +
  theme(legend.position = "bottom")

pcbd + paoa
```

This plot refers to Figure 8 in our paper.

```{r}
# Which percentage of pixels fall outside the AOA?
round(sum(AOA$AOA == 'outside') / length(AOA$AOA),4)*100
```

About `r round(sum(AOA$AOA == 'outside') / length(AOA$AOA),4)*100`\% of cells fall outside the AOA. 

```{r, cache=FALSE, include=FALSE}
# write_stars(cbd_glm_st, "results/haard_CBD.tif")
# write_stars(AOA, layer = "AOA", "results/AOA_haard_CBD.tif")
```

---

*From section 4.2 in [Heisig et al. 2022](https://doi.org/10.3390/fire5010029):*

> Ridge regression analysis was applied to predict CBD for the Haard forest. CV selected an optimal regularization parameter lambda of 10.7. Overall, model performance was poor which was expected, considering the small number of training samples. A model-R² of 0.59 with a RMSE of 0.054 was reported. Independent validation samples produced a higher R² of 0.73, while RMSE degraded to 0.069. Although R² is acceptable, RMSEs are large, considering a CBD training sample mean of 0.095. Differences in model performance and validation scores indicate the introduction of bias by ridge regression.
> Variable importance scores indicated strong dependence on LiDAR-derived vertical structure metrics and optical predictor data. The most relevant predictors included $C_{25}$, $B05_{p10}$, $Z_{p20}$, ${NDVI}_{p90}$, $B04_{p90}$, and $DEM$. Further, the nine next relevant variables in the ranking, $Z_{iqr}$, $Z_{pcum80}$, and $Z_{p65,...,95}$, all describe vegetation structure in the upper third of the tree. This coincides with relative heights at which CBD can be found at the maximum.
> We tested adding training samples from NFI plots (n = 15) but were not able to improve the model. On average, their derived CBD values were significantly smaller than the existing values based on field sampling. NFI surveys include records of species and CH among many other observations. However, they do not include CBH. Supplementing NFI tree lists, for example, with LiDAR-derived CBH at 10 m spatial resolution, is rather inaccurate, especially when considering plots with heterogeneous species composition, age, and vertical structure.
> Spatial prediction and AOA for CBD are shown in Figure 8. CBD values range from 0 to 0.3. They roughly follow the tree species classification, while higher densities can be observed for pine than for beech and red oak. Anomalies in CBD within homogeneous patches dominated by a single species are related to structural differences. Considering only forested pixels, 20% fall outside the AOA. This may again be explained by the low number of training samples. A significant portion is located in areas with steeper slopes.